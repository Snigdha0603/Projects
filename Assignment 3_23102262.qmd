---
title: "Assignment_3"
format: html
editor: visual
---

# Automobile- Sales, Insurance and Review Analysis {#sec-automobile--sales-insurance-and-review-analysis}

## Loading Dataset {#sec-loading-dataset style="Dark Blue"}

Loading the following dataset

1.  Car_sales dataset from assignment 1 where we talked about the sales data.\
    Kaggle link: <https://www.kaggle.com/datasets/missionjee/car-sales-report>

2.  Insurance Claims dataset\
    Kaggle Link: <https://www.kaggle.com/datasets/buntyshah/auto-insurance-claims-data>

3.  Dataset including Reviews given by customers

    Kaggle Link:<https://www.kaggle.com/datasets/shreemunpranav/edmunds-car-review>

```{r}
library(readxl)
Car_Sales<- read_excel("D:/Snigdha/ireland/Class/Applied Analytics in Business and Society/Assignment 3/Car_Sales_new.xlsx")
View(Car_Sales)

Insurance <- read.csv("D:/Snigdha/ireland/Class/Applied Analytics in Business and Society/Assignment 3/insurance_claims_new.csv")
View(Insurance)

Review <- read_excel("D:/Snigdha/ireland/Class/Applied Analytics in Business and Society/Assignment 3/Review_new.xlsx")
View(Review)

```

## Data Pre-processing

Here we are converting the data into lowercase to help in joining the datasets using inner-join and composite key - "Company" & "Model" of the car.

```{r}
library(dplyr)

# Convert the columns to the same case
Insurance$auto_make <- tolower(Insurance$auto_make)
Review$Company <- tolower(Review$Company)
Insurance$auto_model <- tolower(Insurance$auto_model)
Review$Model <- tolower(Review$Model)
Car_Sales$Company <- tolower(Car_Sales$Company)
Car_Sales$Model <- tolower(Car_Sales$Model)

merged_data <- inner_join(Car_Sales, Review, by = c("Company", "Model"))
View(merged_data)

result <- inner_join(merged_data, Insurance, by = c("Company" = "auto_make", "Model" = "auto_model"), relationship = "many-to-many")

str(result)
```

Loading Libraries for Modelling

```{r}
# Load required libraries


library(tidyverse) # For data manipulation
library(lubridate) # For date manipulation
library(tm)        # For text mining
library(tidyverse) # For data manipulation
library(lubridate) # For date manipulation
library(topicmodels) # For topic modeling
```

## Performing EDA

The "result" dataframe, comprising sales data, insurance data, and car reviews, underwent exploratory data analysis (EDA) to gain insights and prepare for subsequent analysis. Summary statistics offered a snapshot of the dataset's characteristics, while visualizations like histograms and boxplots shed light on distributions and relationships. Handling missing values was recommended, and potential outliers in the 'total_claim_amount' variable were identified using boxplots and the interquartile range method. Variable transformations and class balance checks were suggested for improved modeling accuracy. Overall, the EDA process provided a comprehensive understanding of the dataset's structure and key features, laying the groundwork for further analysis and modeling endeavors.

```{r}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Check if the 'tm' package is installed; if not, install it
if (!requireNamespace("tm", quietly = TRUE)) {
  install.packages("tm", dependencies = TRUE, repos = getOption("repos"))
}


# Load required packages
library(dplyr)
library(ggplot2)
# Load necessary libraries
#install.packages("plotly")

library(plotly)
library(dplyr)


# Summary statistics
summary(result)


# Boxplot of 'total_claim_amount'
boxplot_total_claim_amount <- ggplotly(
  ggplot(result, aes(x = "", y = total_claim_amount)) +
    geom_boxplot(fill = "skyblue", color = "black") +
    labs(title = "Boxplot of Total Claim Amount", y = "Total Claim Amount", x = "")
)

# Boxplot of 'age' by 'fraud_reported'
boxplot_age_fraud_reported <- ggplotly(
  ggplot(result, aes(x = fraud_reported, y = age)) +
    geom_boxplot(fill = "lightgreen") +
    labs(title = "Age Distribution by Fraud Reported", x = "Fraud Reported", y = "Age")
)

# Analysis of Policy Characteristics - Boxplot (Annual Premium by Policy State)
boxplot_plot <- ggplotly(ggplot(result, aes(x = policy_state, y = policy_annual_premium)) + geom_boxplot()) %>% layout(title = "Annual Premium by Policy State")

boxplot_total_claim_amount
boxplot_age_fraud_reported
boxplot_plot

# Handling missing values
# Check for missing values
colSums(is.na(result))



# Function to detect outliers using IQR method
detect_outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  return(x < lower_bound | x > upper_bound)
}

# Outlier detection for 'total_claim_amount'
outliers_total_claim_amount <- detect_outliers(result$total_claim_amount)
outliers_total_claim_amount

dataset <- result

# Load required libraries
library(ggplot2)
library(dplyr)


# Analysis of Insured Occupations - Bar plot (Top 10 occupations)
top_occupations <- head(sort(table(dataset$insured_occupation), decreasing = TRUE), 10)
barplot_plot <- plot_ly(x = names(top_occupations), y = top_occupations, type = "bar", marker = list(color = "lightgreen"))
barplot_plot <- barplot_plot %>% layout(title = "Top 10 Insured Occupations", xaxis = list(title = "Occupation"), yaxis = list(title = "Count"))


# Analysis of Claim Characteristics - Histogram (Total Claim Amount)
histogram_plot <- ggplotly(ggplot(dataset, aes(x = total_claim_amount)) + geom_histogram(binwidth = 1000, fill = "lightblue", color = "black")) %>% layout(title = "Distribution of Total Claim Amount")

# Analysis of Demographic Data - Pie chart (Gender Distribution)
gender_counts <- table(dataset$Gender)
pie_plot_gender <- plot_ly(labels = names(gender_counts), values = gender_counts, type = "pie", marker = list(colors = c("pink", "lightblue")))
pie_plot_gender <- pie_plot_gender %>% layout(title = "Gender Distribution")


# Analysis of Fraud Cases - Pie chart
fraud_counts <- table(dataset$fraud_reported)
pie_plot_fraud <- plot_ly(labels = names(fraud_counts), values = fraud_counts, type = "pie", marker = list(colors = c("red", "lightgreen")))
pie_plot_fraud <- pie_plot_fraud %>% layout(title = "Fraud Reported")

# Arrange plots

barplot_plot
boxplot_plot
histogram_plot
pie_plot_gender
pie_plot_fraud


# Interactive Bar Plot of Incident Types

incident_type_plot <- result %>%
  count(incident_type) %>%
  plot_ly(x = ~incident_type, y = ~n, type = 'bar') %>%
  layout(title = "Distribution of Incident Types",
         xaxis = list(title = "Incident Type"),
         yaxis = list(title = "Count"))

# Interactive Pie Chart of Incident Severity
incident_severity_plot <- result %>%
  count(incident_severity) %>%
  plot_ly(labels = ~incident_severity, values = ~n, type = 'pie') %>%
  layout(title = "Distribution of Incident Severity")

# Display the plots
incident_type_plot
incident_severity_plot

```

## Quantitative Analysis:

## Modelling

\
The provided code snippet performs a generalized linear model (GLM) analysis on the "result" dataframe, which combines sales data, insurance data, and car reviews. Initially, the independent variables (predictors) relevant for the analysis are defined, encompassing various attributes such as gender, annual income, age, and incident-related information. These variables are then filtered from the dataframe, and categorical variables are converted to factors to prepare the data for modeling.

Next, a GLM model is fitted using the Gaussian family and the total claim amount as the response variable. This model aims to explore the relationship between the predictors and the total claim amount, considering the various factors and attributes within the dataset. Finally, a summary of the GLM model is displayed, providing insights into the coefficients, significance levels, and goodness of fit metrics.

The result received is a tibble containing 96 rows and 58 columns, representing the combined dataset's observations and variables. It includes diverse information such as car details, insurance information, reviewer details, incident characteristics, and various other attributes related to each observation. The data appears to be well-structured and rich, offering ample opportunities for further analysis and exploration.

```{r}
#install.packages("tm")
#install.packages("topicmodels")
# Load required libraries
library(tm)
library(dplyr)

# Assuming 'result' is your dataframe

# Define independent variables (predictors)
independent_vars <- c("Gender", "Annual_Income", "age", "policy_annual_premium", 
                      "months_as_customer", "policy_deductable", "capital.gains", 
                      "capital.loss", "incident_hour_of_the_day", "number_of_vehicles_involved",
                      "bodily_injuries", "witnesses", "total_claim_amount", 
                      "policy_state", "incident_type", "collision_type", 
                      "incident_severity", "authorities_contacted", "incident_state", 
                      "property_damage", "insured_sex", "insured_education_level", 
                      "insured_occupation", "insured_hobbies", "insured_relationship", 
                      "police_report_available", "fraud_reported")

# Filter the dataframe to include only relevant variables
glm_data <- result %>%
  select(all_of(independent_vars))

# Convert categorical variables to factors
categorical_vars <- c("Gender", "policy_state", "incident_type", "collision_type", 
                      "incident_severity", "authorities_contacted", "incident_state", 
                      "property_damage", "insured_sex", "insured_education_level", 
                      "insured_occupation", "insured_hobbies", "insured_relationship", 
                      "police_report_available", "fraud_reported")

glm_data[categorical_vars] <- lapply(glm_data[categorical_vars], as.factor)

# Fit the GLM model
glm_model <- glm(total_claim_amount ~ ., data = glm_data, family = gaussian())

# Display summary of the model
summary(glm_model)

```

The output from the generalized linear model (GLM) reveals several significant predictors impacting the total claim amount. Notably, variables such as "capital.gains," "bodily_injuries," "insured_occupationcraft-repair," "insured_occupationprotective-serv," "insured_hobbiesboard-games," "insured_hobbiesmovies," and "insured_hobbiespaintball" show significant coefficients.

Furthermore, the model's goodness of fit can be evaluated through the residual deviance, which indicates a substantial reduction from the null deviance. This reduction suggests that the model explains a considerable portion of the variability in the data.

Despite this, it's important to note that some predictors, such as "incident_typeSingle Vehicle Collision" and "insured_sexMALE," while showing somewhat high p-values, may still have practical significance or contribute in conjunction with other variables.

Overall, this GLM provides valuable insights into the factors influencing total claim amount, allowing for more informed decision-making in risk assessment and claims management within the insurance industry.

## Qualitative Analysis:

## Sentiment Analysis

\
The sentiment analysis conducted on the dataset aimed to assess the sentiment expressed in the reviews provided for different car models from various companies. The output provides a summary of the sentiment scores assigned to each car model within their respective companies.

```{r}
#install.packages("tidytext", dependencies = TRUE)
library(tidytext)

#install.packages("devtools")  # Install devtools package if you haven't already
#devtools::install_github("trinker/afinn")


#install.packages("textdata")
library(textdata)

# Specify the path to the AFINN lexicon file
afinn_file <- "https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-111.txt"

# Read the AFINN lexicon into a data frame
afinn <- read.table(afinn_file, sep="\t", header=FALSE, col.names=c("word", "value"))

# Optionally, you can inspect the structure of the AFINN lexicon data frame
str(afinn)



afinn <- get_sentiments("afinn")

library(dplyr)
# Tokenize the text data
tokenized_data <- result %>%
  unnest_tokens(word, Review)

# Perform sentiment scoring using the AFINN lexicon
sentiment_scores <- tokenized_data %>%
  inner_join(afinn, by = "word") %>%
  group_by(Company, Model) %>%
  summarise(sentiment_score = sum(value, na.rm = TRUE)) %>%
  ungroup()

# Display the sentiment scores
head(sentiment_scores)

library(plotly)

plotly_obj <- ggplot(data = sentiment_scores, aes(x = Company, y = sentiment_score, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Sentiment Analysis of Car Reviews",
       x = "Company",
       y = "Sentiment Score",
       fill = "Model") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Convert ggplot object to plotly
plotly_obj <- ggplotly(plotly_obj)

# Print the interactive plot
plotly_obj


```

Here's an explanation of the output:

1.  **Company:** This column represents the name of the car company associated with the respective car model.

2.  **Model:** This column denotes the specific car model for which the sentiment score was calculated.

3.  **Sentiment Score:** This column displays the sentiment score assigned to each car model based on the reviews. The sentiment score indicates the overall sentiment expressed in the reviews for that particular car model.

    -   A positive score indicates a generally positive sentiment expressed in the reviews.

    -   A negative score indicates a generally negative sentiment expressed in the reviews.

    -   A score of zero suggests a neutral sentiment or an equal balance of positive and negative sentiments in the reviews.

Based on the sentiment scores provided in the output:

-   **Chevrolet Malibu:** The reviews for this model have a sentiment score of 51, indicating a somewhat positive sentiment overall.

-   **Honda Accord:** The sentiment score for this model is -126, suggesting a predominantly negative sentiment in the reviews.

-   **Jeep Wrangler:** This model has a sentiment score of 255, indicating a highly positive sentiment in the reviews.

-   **Nissan Pathfinder:** The sentiment score is 0, suggesting a neutral sentiment or an equal balance of positive and negative sentiments in the reviews.

-   **Toyota Camry:** The sentiment score for this model is 104, indicating a relatively positive sentiment in the reviews.

-   **Volkswagen Jetta:** This model has a sentiment score of 160, suggesting a highly positive sentiment in the reviews.

The analysis provides insights into how customers perceive different car models from various companies based on the sentiment expressed in their reviews. Positive sentiment scores indicate that customers generally view the respective car models positively, while negative sentiment scores suggest a negative perception.

## Word Cloud

The code provided conducts text processing and analysis to generate a word cloud depicting the frequency of incident types derived from the dataset. Initially, the incident type data is extracted and preprocessed to ensure consistency and accuracy. Subsequently, a term-document matrix is created to represent the text data in a structured format. This matrix is then used to calculate word frequencies, identifying the most common incident types. Finally, the **`wordcloud`** function is employed to generate a visual representation of these frequencies, with larger and bolder words indicating higher occurrence. This word cloud serves as a concise yet informative summary, facilitating quick insights into the prevalent incident types within the dataset.

```{r}
# Install and load the required packages
#install.packages("wordcloud")
library(wordcloud)

#install.packages("RColorBrewer")
library(RColorBrewer)


#install.packages(tm)
library(tm)

# Create a vector containing only the text
text <- result$incident_type  # Replace "Description" with the name of your desired text column

# Create a corpus  
docs <- Corpus(VectorSource(text))

# Load the dplyr package
library(dplyr)

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))

print(docs)


dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)


# Load the wordcloud package
library(wordcloud)

# Set seed for reproducibility
set.seed(1234)

# Create the word cloud
# Create the word cloud with adjusted scale parameter
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words = 50, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"), scale = c(4.5, 1.25))

```

Based on the word cloud generated from the incident types, the most frequent words are:

1.  **Collision**

2.  **Vehicle**

3.  **Theft**

4.  **Car Parked**

These words represent the most common incident types mentioned in the dataset, with "Collision" being the most prevalent. The word cloud provides a visual representation of the frequency of occurrence of these incident types, with larger and bolder words indicating higher frequencies.

The priority of incident types can be inferred based on their frequency in the dataset. In this case, "Collision" appears to be the most frequent incident type, followed by "Vehicle," "Theft," and "Car Parked." This suggests that collisions are the most common type of incident recorded in the dataset, followed by incidents related to vehicles, theft, and parked cars, respectively.

## Geographical data analysis

We conducted geoplotting to visualize the locations of incidents recorded in the insurance claim data. Leveraging the city and state information provided in the dataset, we approximated the latitude and longitude coordinates for each incident location. These coordinates were then utilized to pinpoint the exact city of the incidents on a dynamic map. By employing the Leaflet library in R, we accurately marked each incident location on the map, providing a comprehensive geographical representation of the insurance claim data.

```{r}
# Install and load ggmap package
#install.packages("ggmap")
library(ggmap)


#install.packages("osmdata")
library(osmdata)

#install.packages("tmaptools")
library(tmaptools)


# Combine city and state columns into a unique address string in the result dataset
addresses <- paste(result$incident_city, result$incident_state, sep = ", ")

# Geocode addresses using the OSM Nominatim API
geocoded <- geocode_OSM(addresses)

# Create a new column in result for the address
result$address <- paste(result$incident_city, result$incident_state, sep = ", ")

library(dplyr)

# Rename columns in geocoded dataset to match result dataset
library(dplyr)

# Rename columns in geocoded dataset to match result dataset
geocoded <- geocoded %>%
  rename(incident_address = query)

# Perform inner join based on incident_address
result_with_geo <- inner_join(result, geocoded, by = c("address" = "incident_address"), relationship =  "many-to-many")


# View the resulting data set
head(result_with_geo)

install.packages("leaflet")

# Load required packages
library(leaflet)

result_with_geo$latitude <- as.numeric(result_with_geo$lat)
result_with_geo$longitude <- as.numeric(result_with_geo$lon)

# Create Leaflet map with adjusted zoom level
map <- leaflet(data = result_with_geo) %>%
  addTiles() %>%  
  addMarkers(lng = ~longitude, lat = ~latitude, popup = ~incident_location) %>% 
  setView(lng = mean(result_with_geo$longitude), lat = mean(result_with_geo$latitude), zoom = 5)  # Adjust zoom level as needed

# Print the map
map

```

## Mermaid Code:

```{mermaid}
flowchart TB
    subgraph "Car_Sales Dataset"
        Car_Sales --> Merged_Dataset
    end
    subgraph "Insurance Dataset"
        Insurance --> Merged_Dataset
    end
    subgraph "Review Dataset"
        Review --> Merged_Dataset
    end
    Merged_Dataset --> Analysis

```

This Mermaid flowchart describes the merging of multiple datasets into one combined dataset and then the subsequent analysis performed on the merged dataset.

1.  The "Car_Sales Dataset", "Insurance Dataset", and "Review Dataset" are represented as separate subgraphs, indicating the individual datasets.

2.  Each dataset is connected to the "Merged_Dataset" node, signifying the merging of these datasets into one combined dataset.

3.  Finally, the "Merged_Dataset" node is linked to the "Analysis" node, indicating that various analyses or operations are performed on the merged dataset.

Overall, the flowchart provides a visual representation of the process of merging multiple datasets and performing analysis on the combined dataset.
