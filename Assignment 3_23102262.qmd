---
title: "Assignment_3"
format: html
editor: visual
---

# Automobile- Sales, Insurance and Review Analysis {#sec-automobile--sales-insurance-and-review-analysis}

## Loading Dataset {#sec-loading-dataset style="Dark Blue"}

Loading the following dataset

1.  Car_sales dataset from assignment 1 where we talked about the sales data.\
    Kaggle link: <https://www.kaggle.com/datasets/missionjee/car-sales-report>

2.  Insurance Claims dataset\
    Kaggle Link: <https://www.kaggle.com/datasets/buntyshah/auto-insurance-claims-data>

3.  Dataset including Reviews given by customers

    Kaggle Link:

```{r}
library(readxl)
Car_Sales<- read_excel("D:/Snigdha/ireland/Class/Applied Analytics in Business and Society/Assignment 3/Car_Sales_new.xlsx")
View(Car_Sales)

Insurance <- read.csv("D:/Snigdha/ireland/Class/Applied Analytics in Business and Society/Assignment 3/insurance_claims_new.csv")
View(Insurance)

Review <- read_excel("D:/Snigdha/ireland/Class/Applied Analytics in Business and Society/Assignment 3/Review_new.xlsx")
View(Review)

```

## Data Pre-processing

Here we are converting the data into lowercase to help in joining the datasets using inner-join and composite key - "Company" & "Model" of the car.

```{r}
library(dplyr)

# Convert the columns to the same case
Insurance$auto_make <- tolower(Insurance$auto_make)
Review$Company <- tolower(Review$Company)
Insurance$auto_model <- tolower(Insurance$auto_model)
Review$Model <- tolower(Review$Model)
Car_Sales$Company <- tolower(Car_Sales$Company)
Car_Sales$Model <- tolower(Car_Sales$Model)

merged_data <- inner_join(Car_Sales, Review, by = c("Company", "Model"))
View(merged_data)

result <- inner_join(merged_data, Insurance, by = c("Company" = "auto_make", "Model" = "auto_model"), relationship = "many-to-many")

str(result)
```

Loading Libraries for Modelling

```{r}
# Load required libraries


library(tidyverse) # For data manipulation
library(lubridate) # For date manipulation
library(tm)        # For text mining
library(tidyverse) # For data manipulation
library(lubridate) # For date manipulation
library(topicmodels) # For topic modeling
```

## Performing EDA

The "result" dataframe, comprising sales data, insurance data, and car reviews, underwent exploratory data analysis (EDA) to gain insights and prepare for subsequent analysis. Summary statistics offered a snapshot of the dataset's characteristics, while visualizations like histograms and boxplots shed light on distributions and relationships. Handling missing values was recommended, and potential outliers in the 'total_claim_amount' variable were identified using boxplots and the interquartile range method. Variable transformations and class balance checks were suggested for improved modeling accuracy. Overall, the EDA process provided a comprehensive understanding of the dataset's structure and key features, laying the groundwork for further analysis and modeling endeavors.

```{r}
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

# Check if the 'tm' package is installed; if not, install it
if (!requireNamespace("tm", quietly = TRUE)) {
  install.packages("tm", dependencies = TRUE, repos = getOption("repos"))
}


# Load required packages
library(dplyr)
library(ggplot2)


# Summary statistics
summary(result)

# Data visualization
# Boxplot of 'total_claim_amount'
ggplot(result, aes(x = "", y = total_claim_amount)) +
  geom_boxplot(fill = "skyblue", color = "black") +
  labs(title = "Boxplot of Total Claim Amount",
       y = "Total Claim Amount", x = "") 

# Boxplot of 'age' by 'fraud_reported'
ggplot(result, aes(x = fraud_reported, y = age)) +
  geom_boxplot(fill = "lightgreen") +
  labs(title = "Age Distribution by Fraud Reported",
       x = "Fraud Reported",
       y = "Age")

# Scatterplot of 'total_claim_amount' vs 'policy_annual_premium'
ggplot(result, aes(x = policy_annual_premium, y = total_claim_amount)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Total Claim Amount vs Policy Annual Premium",
       x = "Policy Annual Premium",
       y = "Total Claim Amount")

# Handling missing values
# Check for missing values
colSums(is.na(result))



# Function to detect outliers using IQR method
detect_outliers <- function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  return(x < lower_bound | x > upper_bound)
}

# Outlier detection for 'total_claim_amount'
outliers_total_claim_amount <- detect_outliers(result$total_claim_amount)
outliers_total_claim_amount

dataset <- result

# Load required libraries
library(ggplot2)
library(dplyr)

# Analysis of Incident Types - Bar plot
incident_type_counts <- table(dataset$incident_type)
barplot(incident_type_counts, main = "Incident Type Distribution", xlab = "Incident Type", ylab = "Count", col = "skyblue")

# Analysis of Collision Types - Pie chart
collision_type_counts <- table(dataset$collision_type)
pie(collision_type_counts, main = "Collision Type Distribution", col = rainbow(length(collision_type_counts)), labels = names(collision_type_counts))

# Analysis of Insured Occupations - Bar plot (Top 10 occupations)
top_occupations <- head(sort(table(dataset$insured_occupation), decreasing = TRUE), 10)
barplot(top_occupations, main = "Top 10 Insured Occupations", xlab = "Occupation", ylab = "Count", col = "lightgreen")

# Analysis of Policy Characteristics - Boxplot (Annual Premium by Policy State)
ggplot(dataset, aes(x = policy_state, y = policy_annual_premium)) + geom_boxplot() + labs(title = "Annual Premium by Policy State")

# Analysis of Claim Characteristics - Histogram (Total Claim Amount)
ggplot(dataset, aes(x = total_claim_amount)) + geom_histogram(binwidth = 1000, fill = "lightblue", color = "black") + labs(title = "Distribution of Total Claim Amount")

# Analysis of Demographic Data - Pie chart (Gender Distribution)
gender_counts <- table(dataset$Gender)

# Plot pie chart for gender distribution
pie(gender_counts, 
    main = "Gender Distribution", 
    col = c("pink", "lightblue"),  # Specify colors for each category
    labels = paste(names(gender_counts), ": ", gender_counts),  # Add count labels to the pie chart
    cex = 0.8)  # Adjust label size if needed

# Analysis of Vehicle Characteristics - Bar plot (Model Distribution)
top_models <- head(sort(table(dataset$Model), decreasing = TRUE), 10)
barplot(top_models, main = "Top 10 Vehicle Models", xlab = "Model", ylab = "Count", col = "lightpink")

# Analysis of Fraud Cases - Pie chart
fraud_counts <- table(dataset$fraud_reported)

# Plot pie chart for gender distribution
pie(fraud_counts, 
    main = "Fraud Reported", 
    col = c("red", "lightgreen"),  # Specify colors for each category
    labels = paste(names(fraud_counts), ": ", fraud_counts),  # Add count labels to the pie chart
    cex = 0.8)  # Adjust label size if needed

# Temporal Analysis - Time series plot (Incidents Over Time)
incidents_over_time <- table(as.Date(dataset$incident_date))
plot(incidents_over_time, type = "o", col = "blue", xlab = "Date", ylab = "Incidents Count", main = "Incidents Over Time")

# Geographical Analysis - Bar plot (Incident State Distribution)
incident_state_counts <- table(dataset$incident_state)
barplot(incident_state_counts, main = "Incident State Distribution", xlab = "State", ylab = "Count", col = "lightyellow")

```

## Quantitative Analysis:

## Modelling

\
The provided code snippet performs a generalized linear model (GLM) analysis on the "result" dataframe, which combines sales data, insurance data, and car reviews. Initially, the independent variables (predictors) relevant for the analysis are defined, encompassing various attributes such as gender, annual income, age, and incident-related information. These variables are then filtered from the dataframe, and categorical variables are converted to factors to prepare the data for modeling.

Next, a GLM model is fitted using the Gaussian family and the total claim amount as the response variable. This model aims to explore the relationship between the predictors and the total claim amount, considering the various factors and attributes within the dataset. Finally, a summary of the GLM model is displayed, providing insights into the coefficients, significance levels, and goodness of fit metrics.

The result received is a tibble containing 96 rows and 58 columns, representing the combined dataset's observations and variables. It includes diverse information such as car details, insurance information, reviewer details, incident characteristics, and various other attributes related to each observation. The data appears to be well-structured and rich, offering ample opportunities for further analysis and exploration.

```{r}
#install.packages("tm")
#install.packages("topicmodels")
# Load required libraries
library(tm)
library(dplyr)

# Assuming 'result' is your dataframe

# Define independent variables (predictors)
independent_vars <- c("Gender", "Annual_Income", "age", "policy_annual_premium", 
                      "months_as_customer", "policy_deductable", "capital.gains", 
                      "capital.loss", "incident_hour_of_the_day", "number_of_vehicles_involved",
                      "bodily_injuries", "witnesses", "total_claim_amount", 
                      "policy_state", "incident_type", "collision_type", 
                      "incident_severity", "authorities_contacted", "incident_state", 
                      "property_damage", "insured_sex", "insured_education_level", 
                      "insured_occupation", "insured_hobbies", "insured_relationship", 
                      "police_report_available", "fraud_reported")

# Filter the dataframe to include only relevant variables
glm_data <- result %>%
  select(all_of(independent_vars))

# Convert categorical variables to factors
categorical_vars <- c("Gender", "policy_state", "incident_type", "collision_type", 
                      "incident_severity", "authorities_contacted", "incident_state", 
                      "property_damage", "insured_sex", "insured_education_level", 
                      "insured_occupation", "insured_hobbies", "insured_relationship", 
                      "police_report_available", "fraud_reported")

glm_data[categorical_vars] <- lapply(glm_data[categorical_vars], as.factor)

# Fit the GLM model
glm_model <- glm(total_claim_amount ~ ., data = glm_data, family = gaussian())

# Display summary of the model
summary(glm_model)

```

The output is from a generalized linear model (GLM) fitted to the data using the Gaussian family. Each row represents a predictor variable, with corresponding estimates, standard errors, t-values, and p-values. The estimates indicate the expected change in the response variable for a one-unit change in the predictor, holding other predictors constant. The t-values and associated p-values assess the significance of each predictor's effect on the response variable. Significance is determined based on the p-values, with smaller values indicating stronger evidence against the null hypothesis. Additionally, the null and residual deviance values provide insights into the goodness of fit of the model. Lower residual deviance values suggest a better fit, indicating that the model explains a larger proportion of the variability in the data. The Akaike Information Criterion (AIC) is used to compare the relative quality of different models, with lower values indicating better-fitting models. Finally, the number of Fisher Scoring iterations indicates the number of iterations required to fit the model. Overall, this output helps assess the significance of predictor variables and the overall fit of the GLM to the data..

## Qualitative Analysis:

## Sentiment Analysis

\
The sentiment analysis conducted on the dataset aimed to assess the sentiment expressed in the reviews provided for different car models from various companies. The output provides a summary of the sentiment scores assigned to each car model within their respective companies.

```{r}
#install.packages("tidytext", dependencies = TRUE)
library(tidytext)

#install.packages("devtools")  # Install devtools package if you haven't already
#devtools::install_github("trinker/afinn")


#install.packages("textdata")
library(textdata)

# Specify the path to the AFINN lexicon file
afinn_file <- "https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-111.txt"

# Read the AFINN lexicon into a data frame
afinn <- read.table(afinn_file, sep="\t", header=FALSE, col.names=c("word", "value"))

# Optionally, you can inspect the structure of the AFINN lexicon data frame
str(afinn)



afinn <- get_sentiments("afinn")

library(dplyr)
# Tokenize the text data
tokenized_data <- result %>%
  unnest_tokens(word, Review)

# Perform sentiment scoring using the AFINN lexicon
sentiment_scores <- tokenized_data %>%
  inner_join(afinn, by = "word") %>%
  group_by(Company, Model) %>%
  summarise(sentiment_score = sum(value, na.rm = TRUE)) %>%
  ungroup()

# Display the sentiment scores
head(sentiment_scores)

library(ggplot2)

# Create bar plot with Company and Model
ggplot(data = sentiment_scores, aes(x = Company, y = sentiment_score, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Sentiment Analysis of Car Reviews",
       x = "Company",
       y = "Sentiment Score",
       fill = "Model") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Here's an explanation of the output:

1.  **Company:** This column represents the name of the car company associated with the respective car model.

2.  **Model:** This column denotes the specific car model for which the sentiment score was calculated.

3.  **Sentiment Score:** This column displays the sentiment score assigned to each car model based on the reviews. The sentiment score indicates the overall sentiment expressed in the reviews for that particular car model.

    -   A positive score indicates a generally positive sentiment expressed in the reviews.

    -   A negative score indicates a generally negative sentiment expressed in the reviews.

    -   A score of zero suggests a neutral sentiment or an equal balance of positive and negative sentiments in the reviews.

Based on the sentiment scores provided in the output:

-   **Chevrolet Malibu:** The reviews for this model have a sentiment score of 51, indicating a somewhat positive sentiment overall.

-   **Honda Accord:** The sentiment score for this model is -126, suggesting a predominantly negative sentiment in the reviews.

-   **Jeep Wrangler:** This model has a sentiment score of 255, indicating a highly positive sentiment in the reviews.

-   **Nissan Pathfinder:** The sentiment score is 0, suggesting a neutral sentiment or an equal balance of positive and negative sentiments in the reviews.

-   **Toyota Camry:** The sentiment score for this model is 104, indicating a relatively positive sentiment in the reviews.

-   **Volkswagen Jetta:** This model has a sentiment score of 160, suggesting a highly positive sentiment in the reviews.

The analysis provides insights into how customers perceive different car models from various companies based on the sentiment expressed in their reviews. Positive sentiment scores indicate that customers generally view the respective car models positively, while negative sentiment scores suggest a negative perception.

## Word Cloud

The code provided conducts text processing and analysis to generate a word cloud depicting the frequency of incident types derived from the dataset. Initially, the incident type data is extracted and preprocessed to ensure consistency and accuracy. Subsequently, a term-document matrix is created to represent the text data in a structured format. This matrix is then used to calculate word frequencies, identifying the most common incident types. Finally, the **`wordcloud`** function is employed to generate a visual representation of these frequencies, with larger and bolder words indicating higher occurrence. This word cloud serves as a concise yet informative summary, facilitating quick insights into the prevalent incident types within the dataset.

```{r}
# Install and load the required packages
#install.packages("wordcloud")
library(wordcloud)

#install.packages("RColorBrewer")
library(RColorBrewer)


#install.packages(tm)
library(tm)

# Create a vector containing only the text
text <- result$incident_type  # Replace "Description" with the name of your desired text column

# Create a corpus  
docs <- Corpus(VectorSource(text))

# Load the dplyr package
library(dplyr)

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))

print(docs)


dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)


# Load the wordcloud package
library(wordcloud)

# Set seed for reproducibility
set.seed(1234)

# Create the word cloud
# Create the word cloud with adjusted scale parameter
wordcloud(words = df$word, freq = df$freq, min.freq = 1, max.words = 50, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"), scale = c(4.5, 1.25))

```

Based on the word cloud generated from the incident types, the most frequent words are:

1.  **Collision**

2.  **Vehicle**

3.  **Theft**

4.  **Car Parked**

These words represent the most common incident types mentioned in the dataset, with "Collision" being the most prevalent. The word cloud provides a visual representation of the frequency of occurrence of these incident types, with larger and bolder words indicating higher frequencies.

The priority of incident types can be inferred based on their frequency in the dataset. In this case, "Collision" appears to be the most frequent incident type, followed by "Vehicle," "Theft," and "Car Parked." This suggests that collisions are the most common type of incident recorded in the dataset, followed by incidents related to vehicles, theft, and parked cars, respectively.

## Geographical data analysis

We conducted geoplotting to visualize the locations of incidents recorded in the insurance claim data. Leveraging the city and state information provided in the dataset, we approximated the latitude and longitude coordinates for each incident location. These coordinates were then utilized to pinpoint the exact locations of the incidents on a dynamic map. By employing the Leaflet library in R, we accurately marked each incident location on the map, providing a comprehensive geographical representation of the insurance claim data.

```{r}
# Install and load ggmap package
#install.packages("ggmap")
library(ggmap)


#install.packages("osmdata")
library(osmdata)

#install.packages("tmaptools")
library(tmaptools)


# Combine city and state columns into a unique address string in the result dataset
addresses <- paste(result$incident_city, result$incident_state, sep = ", ")

# Geocode addresses using the OSM Nominatim API
geocoded <- geocode_OSM(addresses)

# Create a new column in result for the address
result$address <- paste(result$incident_city, result$incident_state, sep = ", ")

library(dplyr)

# Rename columns in geocoded dataset to match result dataset
library(dplyr)

# Rename columns in geocoded dataset to match result dataset
geocoded <- geocoded %>%
  rename(incident_address = query)

# Perform inner join based on incident_address
result_with_geo <- inner_join(result, geocoded, by = c("address" = "incident_address"), relationship =  "many-to-many")


# View the resulting dataset
head(result_with_geo)

install.packages("leaflet")

# Load required packages
library(leaflet)

result_with_geo$latitude <- as.numeric(result_with_geo$lat)
result_with_geo$longitude <- as.numeric(result_with_geo$lon)

# Create Leaflet map with adjusted zoom level
map <- leaflet(data = result_with_geo) %>%
  addTiles() %>%  
  addMarkers(lng = ~longitude, lat = ~latitude, popup = ~incident_location) %>% 
  setView(lng = mean(result_with_geo$longitude), lat = mean(result_with_geo$latitude), zoom = 5)  # Adjust zoom level as needed

# Print the map
map

```
